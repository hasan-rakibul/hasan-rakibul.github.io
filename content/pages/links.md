Title: Useful links

# Academia
- [The Efforless Academic](https://ilyashabanov.substack.com/)
- [Awesome tips](https://github.com/jbhuang0604/awesome-tips)
- [Collection of advice for prospective and current PhD students by Paul Liang](https://github.com/pliang279/awesome-phd-advice)
- ["Some resources I have found helpful" - Leena Mathur](https://l-mathur.github.io/)
- [Calendar. Not to-do lists.](https://deviparikh.medium.com/calendar-in-stead-of-to-do-lists-9ada86a512dd)
- [Collections related to paper writing, publications, etc. by Professor Cornelia Caragea](https://www.cs.uic.edu/~cornelia/links.html)

&nbsp;

# Upcoming conferences
- [**Computer Science conferences timeline (lucjaulmes.github.io)**](https://lucjaulmes.github.io/cfp-timeline/)
- [AI Conference Deadlines (aideadlin.es)](https://aideadlin.es/?sub=ML,CV,CG,NLP,RO,SP,DM,AP,KR,HCI)
- [Upcoming Conferences – ACM SIGCHI](https://sigchi.org/conferences/upcoming-conferences/)
- [AAAI Association for the Advancement of Artificial Intelligence](https://aaai.org/)
- [Conferences | IEEE SWC2023 (ieee-smart-world-congress.org)](https://ieee-smart-world-congress.org/program/overview)

&nbsp;

# Deep learning
- [What should I do when my neural network doesn't learn?](https://stats.stackexchange.com/questions/352036/what-should-i-do-when-my-neural-network-doesnt-learn)
- [What should I do when my neural network doesn't generalize well?](https://stats.stackexchange.com/questions/365778/what-should-i-do-when-my-neural-network-doesnt-generalize-well)

## Natural language processing
- [The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/)
- [A Visual Guide to Using BERT for the First Time](http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/)
- [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](http://jalammar.github.io/illustrated-bert/)
- [Paper Dissected: “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding” Explained](https://datasciencetoday.net/index.php/en-us/nlp/211-paper-dissected-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained)
- [The Illustrated Word2vec](http://jalammar.github.io/illustrated-word2vec/)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [Animated RNN, LSTM and GRU](https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45)
- [Transformer Neural Networks - EXPLAINED! (Attention is all you need)](https://www.youtube.com/watch?v=TQQlZhbC5ps)
- [What is Transformer?](https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04)
- [Pooling Strategies for Downstream Tasks - Utilizing Transformer Representations Efficiently](https://www.kaggle.com/code/rhtsingh/utilizing-transformer-representations-efficiently)